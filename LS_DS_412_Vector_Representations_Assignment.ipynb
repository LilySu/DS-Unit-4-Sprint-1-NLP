{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
    "\n",
    "# Document Representations: Bag-Of-Words"
<<<<<<< HEAD
=======
    "\n",
    "# Vector Representations\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*"
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
   "execution_count": null,
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "id": "hyj-f9FDcVFp",
    "outputId": "5dd045fe-6e4c-458c-e2fc-253c3da9c805"
   },
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /usr/local/lib/python3.6/dist-packages (3.4)\n",
      "Requirement already satisfied, skipping upgrade: singledispatch in /usr/local/lib/python3.6/dist-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
<<<<<<< HEAD
=======
   "outputs": [],
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   "source": [
    "import re\n",
    "import string\n",
    "\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
    "!pip install -U nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize # Sentence Tokenizer\n",
    "from nltk.tokenize import word_tokenize # Word Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
<<<<<<< HEAD
=======
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7bcmqfGXrFG"
   },
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "# 1) (optional) Scrape 100 Job Listings that contain the title \"Data Scientist\" from indeed.com\n",
=======
    "## 1) *Optional:* Scrape 100 Job Listings that contain the title \"Data Scientist\" from indeed.com\n",
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
    "# 1) (optional) Scrape 100 Job Listings that contain the title \"Data Scientist\" from indeed.com\n",
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
    "\n",
    "At a minimum your final dataframe of job listings should contain\n",
    "- Job Title\n",
    "- Job Description\n",
    "\n",
    "If you choose to not to scrape the data, there is a CSV with outdated data in the directory. Remeber, if you scrape Indeed, you're helping yourself find a job. ;)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests-html\n",
      "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
      "Collecting fake-useragent (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
      "Collecting w3lib (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/81/43/9dcf92a77f5f0afe4f4df2407d7289eea01368a08b64bda00dd318ca62a6/w3lib-1.20.0-py2.py3-none-any.whl\n",
      "Collecting pyquery (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/c7/ce8c9c37ab8ff8337faad3335c088d60bed4a35a4bed33a64f0e64fbcf29/pyquery-1.4.0-py2.py3-none-any.whl\n",
      "Collecting bs4 (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/16/a5e8d617994cac605f972523bb25f12e3ff9c30baee29b4a9c50467229d9/pyppeteer-0.0.25.tar.gz (1.2MB)\n",
      "Collecting parse (from requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/c4/c0/324d280a3298cdad806c3fb64eef31aed5c4dbd15b72a309498fb71c6a17/parse-1.12.0.tar.gz\n",
      "Requirement already satisfied: requests in c:\\users\\lilyx\\appdata\\roaming\\python\\python37\\site-packages (from requests-html) (2.21.0)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\lilyx\\appdata\\roaming\\python\\python37\\site-packages (from w3lib->requests-html) (1.11.0)\n",
      "Collecting cssselect>0.7.9 (from pyquery->requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: lxml>=2.1 in c:\\users\\lilyx\\anaconda3\\lib\\site-packages (from pyquery->requests-html) (4.3.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lilyx\\anaconda3\\lib\\site-packages (from bs4->requests-html) (4.7.1)\n",
      "Collecting pyee (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/d8/5608d571ffad3d7de0192b0b3099fe3f38d87c0817ebff3cee19264f0bc2/pyee-6.0.0-py2.py3-none-any.whl\n",
      "Collecting websockets (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/9c/60/f96f535f3354cb6ba5e5c7ab128b1c4802a2d040ee7225e3fe51242816c1/websockets-8.0.2-cp37-cp37m-win_amd64.whl (65kB)\n",
      "Collecting appdirs (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3 in c:\\users\\lilyx\\appdata\\roaming\\python\\python37\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.24.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lilyx\\appdata\\roaming\\python\\python37\\site-packages (from pyppeteer>=0.0.14->requests-html) (4.31.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\lilyx\\appdata\\roaming\\python\\python37\\site-packages (from requests->requests-html) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lilyx\\appdata\\roaming\\python\\python37\\site-packages (from requests->requests-html) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\lilyx\\appdata\\roaming\\python\\python37\\site-packages (from requests->requests-html) (3.0.4)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\lilyx\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4->requests-html) (1.8)\n",
      "Building wheels for collected packages: fake-useragent, bs4, pyppeteer, parse\n",
      "  Building wheel for fake-useragent (setup.py): started\n",
      "  Building wheel for fake-useragent (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\lilyx\\AppData\\Local\\pip\\Cache\\wheels\\5e\\63\\09\\d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\lilyx\\AppData\\Local\\pip\\Cache\\wheels\\a0\\b0\\b2\\4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "  Building wheel for pyppeteer (setup.py): started\n",
      "  Building wheel for pyppeteer (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\lilyx\\AppData\\Local\\pip\\Cache\\wheels\\34\\e0\\5d\\070e22eceecf7ecd5fa4b86bbc18c1c7d0b90e02e9b57f35eb\n",
      "  Building wheel for parse (setup.py): started\n",
      "  Building wheel for parse (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\lilyx\\AppData\\Local\\pip\\Cache\\wheels\\9f\\62\\d1\\c46b7452aa0b2c838080bdd462110cd6c61890151f916aa743\n",
      "Successfully built fake-useragent bs4 pyppeteer parse\n",
      "Installing collected packages: fake-useragent, w3lib, cssselect, pyquery, bs4, pyee, websockets, appdirs, pyppeteer, parse, requests-html\n",
      "Successfully installed appdirs-1.4.3 bs4-0.0.1 cssselect-1.0.3 fake-useragent-0.1.11 parse-1.12.0 pyee-6.0.0 pyppeteer-0.0.25 pyquery-1.4.0 requests-html-0.10.0 w3lib-1.20.0 websockets-8.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests-html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
<<<<<<< HEAD
=======
   "execution_count": null,
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KcYlc1URXhlC"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
    "from urllib.parse import urlencode\n",
    "from requests_html import HTMLSession\n",
    "from multiprocessing.dummy import Pool\n",
    "from itertools import chain\n",
    "\n",
    "class IndeedJobListings:\n",
    "    '''\n",
    "    Multi-theaded Indeed Job Listings Crawler\n",
    "    Usage:\n",
    "    descriptions = IndeedJobListings('Data Scientist', 'Seattle, WA').get_descriptions()\n",
    "    '''\n",
    "    def __init__(self, search_keyword, location, threads=12):\n",
    "        self.threads = threads\n",
    "        self.base_url = 'https://www.indeed.com'\n",
    "        self.query_url = f'{self.base_url}/jobs?' +\\\n",
    "        urlencode({'q': search_keyword, 'l': location})\n",
    "        self.session = HTMLSession()\n",
    "    \n",
    "    def _get_posting_urls(self, url):\n",
    "        doc = self.session.get(url)\n",
    "        posting_urls = [f'{self.base_url}{e.attrs[\"href\"]}' for e in doc.html.find('.jobtitle.turnstileLink')]\n",
    "        return posting_urls\n",
    "\n",
    "    def _get_description_text(self, url):\n",
    "        doc = self.session.get(url)\n",
    "        description_text = doc.html.find('#jobDescriptionText')[0].text\n",
    "        return description_text\n",
    "                        \n",
    "    def get_descriptions(self, pages=1):\n",
    "        list_urls = [self.query_url] + [f'{self.query_url}&start={x*10}'\n",
    "                                        for x in range(1, pages)]\n",
    "        p = Pool(self.threads)\n",
    "        post_urls = chain(*p.map(self._get_posting_urls, list_urls))\n",
    "        descriptions = p.map(self._get_description_text, post_urls)\n",
    "        return descriptions\n",
    "\n",
    "\n",
    "listings = IndeedJobListings('Data Scientist', 'Seattle, WA')\n",
    "descriptions = listings.get_descriptions(pages=10)"
<<<<<<< HEAD
=======
    "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5C4xFZNtX1m2"
   },
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
    "#### 2) Use Spacy to tokenize / clean the listings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(descriptions)"
<<<<<<< HEAD
=======
    "## 2) Use Spacy to tokenize / clean the listings "
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hihope',\n",
       " 'you',\n",
       " 'are',\n",
       " 'doing',\n",
       " 'wellthis',\n",
       " 'is',\n",
       " 'nasim',\n",
       " 'shah',\n",
       " 'from',\n",
       " 'tredence',\n",
       " 'we',\n",
       " 'have',\n",
       " 'an',\n",
       " 'exciting',\n",
       " 'opportunity',\n",
       " 'for',\n",
       " 'those',\n",
       " 'who',\n",
       " 'are',\n",
       " 'passionate',\n",
       " 'about',\n",
       " 'data',\n",
       " 'analytics',\n",
       " 'we',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'data',\n",
       " 'scientist',\n",
       " 'in',\n",
       " 'seattle',\n",
       " 'wa',\n",
       " 'for',\n",
       " 'our',\n",
       " 'company',\n",
       " 'tredence',\n",
       " 'please',\n",
       " 'find',\n",
       " 'the',\n",
       " 'job',\n",
       " 'description',\n",
       " 'belowabout',\n",
       " 'ustredence',\n",
       " 'is',\n",
       " 'a',\n",
       " 'global',\n",
       " 'analytics',\n",
       " 'services',\n",
       " 'and',\n",
       " 'solutions',\n",
       " 'company',\n",
       " 'our',\n",
       " 'capabilities',\n",
       " 'range',\n",
       " 'from',\n",
       " 'data',\n",
       " 'visualization',\n",
       " 'data',\n",
       " 'management',\n",
       " 'to',\n",
       " 'advanced',\n",
       " 'analytics',\n",
       " 'big',\n",
       " 'data',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'our',\n",
       " 'uniqueness',\n",
       " 'is',\n",
       " 'in',\n",
       " 'bringing',\n",
       " 'the',\n",
       " 'right',\n",
       " 'mix',\n",
       " 'of',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'business',\n",
       " 'analytics',\n",
       " 'to',\n",
       " 'create',\n",
       " 'sustainable',\n",
       " 'whitebox',\n",
       " 'solutions',\n",
       " 'that',\n",
       " 'are',\n",
       " 'transitioned',\n",
       " 'to',\n",
       " 'our',\n",
       " 'clients',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'engagement',\n",
       " 'we',\n",
       " 'do',\n",
       " 'this',\n",
       " 'cost',\n",
       " 'effectively',\n",
       " 'using',\n",
       " 'a',\n",
       " 'global',\n",
       " 'execution',\n",
       " 'model',\n",
       " 'leveraging',\n",
       " 'our',\n",
       " 'clients',\n",
       " 'existing',\n",
       " 'technology',\n",
       " 'and',\n",
       " 'data',\n",
       " 'assets',\n",
       " 'we',\n",
       " 'also',\n",
       " 'come',\n",
       " 'in',\n",
       " 'with',\n",
       " 'strong',\n",
       " 'ip',\n",
       " 'and',\n",
       " 'prebuilt',\n",
       " 'analytics',\n",
       " 'solutions',\n",
       " 'in',\n",
       " 'data',\n",
       " 'mining',\n",
       " 'business',\n",
       " 'intelligence',\n",
       " 'and',\n",
       " 'big',\n",
       " 'datapermanent',\n",
       " 'position',\n",
       " 'job',\n",
       " 'responsibilitieslead',\n",
       " 'and',\n",
       " 'manage',\n",
       " 'independently',\n",
       " 'the',\n",
       " 'onsiteoffshore',\n",
       " 'relation',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'adding',\n",
       " 'value',\n",
       " 'to',\n",
       " 'the',\n",
       " 'clientengage',\n",
       " 'with',\n",
       " 'clients',\n",
       " 'and',\n",
       " 'business',\n",
       " 'partners',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'their',\n",
       " 'requirement',\n",
       " 'identify',\n",
       " 'their',\n",
       " 'challengesprovide',\n",
       " 'clear',\n",
       " 'business',\n",
       " 'context',\n",
       " 'and',\n",
       " 'deliver',\n",
       " 'actionable',\n",
       " 'insights',\n",
       " 'and',\n",
       " 'recommendations',\n",
       " 'by',\n",
       " 'designing',\n",
       " 'analytical',\n",
       " 'solutions',\n",
       " 'and',\n",
       " 'frameworks',\n",
       " 'in',\n",
       " 'collaboration',\n",
       " 'with',\n",
       " 'the',\n",
       " 'offshore',\n",
       " 'team',\n",
       " 'in',\n",
       " 'indiaput',\n",
       " 'together',\n",
       " 'a',\n",
       " 'solution',\n",
       " 'architecture',\n",
       " 'that',\n",
       " 'is',\n",
       " 'scalable',\n",
       " 'reusable',\n",
       " 'efficient',\n",
       " 'and',\n",
       " 'effectivepresent',\n",
       " 'results',\n",
       " 'insights',\n",
       " 'and',\n",
       " 'recommendations',\n",
       " 'to',\n",
       " 'senior',\n",
       " 'management',\n",
       " 'with',\n",
       " 'an',\n",
       " 'emphasis',\n",
       " 'on',\n",
       " 'the',\n",
       " 'now',\n",
       " 'what',\n",
       " 'ie',\n",
       " 'business',\n",
       " 'impactbuild',\n",
       " 'engaging',\n",
       " 'rapport',\n",
       " 'with',\n",
       " 'client',\n",
       " 'leadership',\n",
       " 'through',\n",
       " 'relevant',\n",
       " 'conversations',\n",
       " 'and',\n",
       " 'genuine',\n",
       " 'business',\n",
       " 'recommendations',\n",
       " 'that',\n",
       " 'impact',\n",
       " 'the',\n",
       " 'growth',\n",
       " 'and',\n",
       " 'profitability',\n",
       " 'of',\n",
       " 'the',\n",
       " 'organizationbuild',\n",
       " 'and',\n",
       " 'grow',\n",
       " 'the',\n",
       " 'organization',\n",
       " 'through',\n",
       " 'presales',\n",
       " 'operations',\n",
       " 'and',\n",
       " 'training',\n",
       " 'enablementskills',\n",
       " 'and',\n",
       " 'qualifications3',\n",
       " 'to',\n",
       " '5',\n",
       " 'years',\n",
       " 'of',\n",
       " 'database',\n",
       " 'experience',\n",
       " 'with',\n",
       " 'advanced',\n",
       " 'sql',\n",
       " 'skillsstrong',\n",
       " 'business',\n",
       " 'skills',\n",
       " 'strong',\n",
       " 'sql',\n",
       " 'customermarketing',\n",
       " 'analytics',\n",
       " 'experiencesql',\n",
       " 'advanced',\n",
       " 'hive',\n",
       " 'sql',\n",
       " 'incl',\n",
       " 'window',\n",
       " 'functions',\n",
       " 'advanced',\n",
       " 'aggregations',\n",
       " 'etcpython',\n",
       " 'scripting',\n",
       " 'reusable',\n",
       " 'classes',\n",
       " 'functionsstatistics',\n",
       " 'in',\n",
       " 'addition',\n",
       " 'to',\n",
       " 'what',\n",
       " 'we',\n",
       " 'current',\n",
       " 'have',\n",
       " 'also',\n",
       " 'add',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'test',\n",
       " 'control',\n",
       " 'analysis',\n",
       " 'confidence',\n",
       " 'intervals',\n",
       " 'bootstrapability',\n",
       " 'to',\n",
       " 'research',\n",
       " 'and',\n",
       " 'manipulate',\n",
       " 'complex',\n",
       " 'and',\n",
       " 'large',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'both',\n",
       " 'distributed',\n",
       " 'and',\n",
       " 'nondistributedability',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'complex',\n",
       " 'business',\n",
       " 'problem',\n",
       " 'through',\n",
       " 'a',\n",
       " 'blend',\n",
       " 'of',\n",
       " 'logical',\n",
       " 'and',\n",
       " 'creative',\n",
       " 'thinking',\n",
       " 'mental',\n",
       " 'ambidexteritystrong',\n",
       " 'communication',\n",
       " 'skills',\n",
       " 'with',\n",
       " 'excellent',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'synthesize',\n",
       " 'complex',\n",
       " 'informationworking',\n",
       " 'experience',\n",
       " 'in',\n",
       " 'retail',\n",
       " 'ecommerce',\n",
       " 'merchandising',\n",
       " 'marketing',\n",
       " 'will',\n",
       " 'be',\n",
       " 'an',\n",
       " 'added',\n",
       " 'advantage',\n",
       " 'bachelors',\n",
       " 'in',\n",
       " 'engineering',\n",
       " 'or',\n",
       " 'masters',\n",
       " 'in',\n",
       " 'statistics',\n",
       " 'economics',\n",
       " 'business',\n",
       " 'administrationregardsnasim',\n",
       " 'shahassociate',\n",
       " 'manager',\n",
       " 'us',\n",
       " 'recruitment',\n",
       " 'tredence',\n",
       " 'connect',\n",
       " 'the',\n",
       " 'dots',\n",
       " 'wwwtredencecomdirect',\n",
       " '4088313758follow',\n",
       " 'us',\n",
       " 'on',\n",
       " 'f',\n",
       " 't',\n",
       " 'ininc5000',\n",
       " 'for',\n",
       " 'the',\n",
       " '3rd',\n",
       " 'consecutive',\n",
       " 'year',\n",
       " 'economic',\n",
       " 'times',\n",
       " 'bootstrap',\n",
       " 'champ2018job',\n",
       " 'type',\n",
       " 'fulltimesalary',\n",
       " '10000000',\n",
       " 'to',\n",
       " '13000000',\n",
       " 'yearexperiencedata',\n",
       " 'analytics',\n",
       " '3',\n",
       " 'years',\n",
       " 'preferrededucationbachelors',\n",
       " 'preferredadditional',\n",
       " 'compensationother',\n",
       " 'forms']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#substitute anything that is not in the regular expression pattern with empty string, \n",
    "#do with sample string\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Parses a string into a list of semantic units (words)\n",
    "\n",
    "    Args:\n",
    "        text (str): The string that the function will tokenize.\n",
    "\n",
    "    Returns:\n",
    "        list: tokens parsed out by the mechanics of your choice\n",
    "    \"\"\"\n",
    "    tokens = re.sub(r'[^a-zA-Z ^0-9]', '', text)\n",
    "    tokens = tokens.lower().split()\n",
    "    return tokens\n",
    "\n",
    "tokenize(descriptions[0])"
<<<<<<< HEAD
=======
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhUHuMr-X-II"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lgCZNL_YycP"
   },
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "#### 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
=======
    "## 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
    "#### 3) Use Scikit-Learn's CountVectorizer to get word counts for each listing."
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 46,
=======
   "execution_count": null,
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
   "execution_count": 46,
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2PZ8Pj_YxcF"
   },
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>03</th>\n",
       "      <th>06</th>\n",
       "      <th>0660</th>\n",
       "      <th>07</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>...</th>\n",
       "      <th>your</th>\n",
       "      <th>yourself</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yrs</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zillow</th>\n",
       "      <th>zone</th>\n",
       "      <th>zulily</th>\n",
       "      <th>zuniversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4508 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  01  03  06  0660  07  09  10  100  ...  your  yourself  youtube  \\\n",
       "0   2    2   0   0   0     0   0   0   0    1  ...     0         0        0   \n",
       "1   0    0   0   0   0     0   0   0   0    0  ...     1         0        0   \n",
       "2   0    0   0   0   0     0   0   0   0    0  ...     1         0        0   \n",
       "3   0    0   0   0   0     0   0   0   0    0  ...     0         0        0   \n",
       "4   0    0   0   0   0     0   0   0   0    0  ...     2         0        0   \n",
       "\n",
       "   yrs  zealous  zelda  zillow  zone  zulily  zuniversity  \n",
       "0    0        0      0       0     0       0            0  \n",
       "1    0        0      0       0     0       0            0  \n",
       "2    0        0      0       0     0       0            0  \n",
       "3    0        0      0       0     0       0            0  \n",
       "4    0        0      0       0     0       0            0  \n",
       "\n",
       "[5 rows x 4508 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "df = vect.fit_transform(descriptions)\n",
    "\n",
    "df = pd.DataFrame(df.todense(), columns=vect.get_feature_names())\n",
    "df.head()"
<<<<<<< HEAD
=======
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zo1iH_UeY7_n"
   },
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
    "#### 4) Visualize the most common word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     74\n",
       "2     32\n",
       "1     29\n",
       "3     25\n",
       "4     18\n",
       "5      3\n",
       "10     1\n",
       "6      1\n",
       "Name: your, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['your'].value_counts()"
<<<<<<< HEAD
=======
    "## 4) Visualize the most common word counts"
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 39,
=======
   "execution_count": null,
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
   "execution_count": 39,
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5LB00uyZKV5"
   },
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sort_values() missing 1 required positional argument: 'by'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-2a91cdc59296>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: sort_values() missing 1 required positional argument: 'by'"
     ]
    }
   ],
   "source": [
    "df.sort_values(ascending=False)[:5]"
<<<<<<< HEAD
=======
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwFsTqrVZMYi"
   },
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    " #### 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
=======
    "## 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
    " #### 5) Use Scikit-Learn's tfidfVectorizer to get a TF-IDF feature matrix"
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-gx2gZCbl5Np"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    " #### 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
=======
    "## 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
    " #### 6) Create a NearestNeighbor Model. Write the description of your ideal datascience job and query your job listings. "
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "raise Exception(\"\\nThis task is not complete. \\nReplace this line with your code for the task.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiDfTWceoRkH"
   },
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "#### Stretch Goals\n",
=======
    "## Stretch Goals\n",
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
    "#### Stretch Goals\n",
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
    "\n",
    " - Try different visualizations for words and frequencies - what story do you want to tell with the data?\n",
    " - Scrape Job Listings for the job title \"Data Analyst\". How do these differ from Data Scientist Job Listings\n",
    " - Try and identify requirements for experience specific technologies that are asked for in the job listings. How are those distributed among the job listings?\n",
    " - Use a clustering algorithm to cluster documents by their most important terms. Do the clusters reveal any common themes?\n",
<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
    "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
<<<<<<< HEAD
=======
    "  - **Hint:** K-means might not be the best algorithm for this. Do a little bit of research to see what might be good for this. Also, remember that algorithms that depend on Euclidean distance break down with high dimensional data.\n",
    " - Create a labeled dataset - which jobs will you apply for? Train a model to select the jobs you are most likely to apply for. :) "
   ]
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_422_BOW_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
<<<<<<< HEAD
<<<<<<< HEAD
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
=======
   "display_name": "U4-S1-NLP (Python 3)",
   "language": "python",
   "name": "u4-s1-nlp"
>>>>>>> 95a9ec431168796939059a07f8361caae75c2225
=======
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
>>>>>>> 8b9eebf3513d122565099d9128a6221ecd03309b
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
